{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Satvinder Singh Panesar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code in Python 3\n",
    "# Note: Please follow the execution sequence to avoid errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Titanic Data Code taken from \n",
    "# https://creativedata.atlassian.net/wiki/spaces/SAP/pages/83237142/Pyspark+-+Tutorial+based+on+Titanic+Dataset\n",
    "# Import packages\n",
    "import findspark\n",
    "findspark.init()\n",
    "import time\n",
    "import pyspark\n",
    "import os\n",
    "import csv\n",
    "from numpy import array\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "# Creating Spark environment\n",
    "os.environ[\"HADOOP_USER_NAME\"] = \"hdfs\"\n",
    "os.environ[\"PYTHON_VERSION\"] = \"3.5.2\"\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic using mllib module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF takes 7 s\n",
      "Area under PR = 0.5888035357255603\n",
      "Area under ROC = 0.7889872717210846\n"
     ]
    }
   ],
   "source": [
    "# Reading from the hdfs, removing the header\n",
    "trainTitanic = sc.textFile(\"train.csv\")\n",
    "trainHeader = trainTitanic.first()\n",
    "trainTitanic = trainTitanic.filter(lambda line: line != trainHeader).mapPartitions(lambda x: csv.reader(x))\n",
    "trainTitanic.first()\n",
    " \n",
    "# Data preprocessing\n",
    "def sexTransformMapper(elem):\n",
    "    '''Function which transform \"male\" into 1 and else things into 0\n",
    "    - elem : string\n",
    "    - return : vector\n",
    "    '''\n",
    "     \n",
    "    if elem == 'male' :\n",
    "        return [0]\n",
    "    else :\n",
    "        return [1]\n",
    "    \n",
    "# Data Transformations and filter lines with empty strings\n",
    "trainTitanic=trainTitanic.map(lambda line: line[1:3]+sexTransformMapper(line[4])+line[5:11])\n",
    "trainTitanic=trainTitanic.filter(lambda line: line[3] != '' ).filter(lambda line: line[4] != '' )\n",
    "trainTitanic.take(10)\n",
    " \n",
    "# creating \"labeled point\" rdds specific to MLlib \"(label (v1, v2...vp])\"\n",
    "trainTitanicLP=trainTitanic.map(lambda line: LabeledPoint(line[0],[line[1:5]]))\n",
    "trainTitanicLP.first()\n",
    " \n",
    "# splitting dataset into train and test set\n",
    "(trainData, testData) = trainTitanicLP.randomSplit([0.7, 0.3])\n",
    " \n",
    "# Random forest : same parameters as sklearn (?)\n",
    "from pyspark.mllib.tree import RandomForest\n",
    " \n",
    "time_start=time.time()\n",
    "model_rf = RandomForest.trainClassifier(trainData, numClasses = 2,\n",
    "        categoricalFeaturesInfo = {}, numTrees = 100,\n",
    "        featureSubsetStrategy='auto', impurity='gini', maxDepth=12,\n",
    "        maxBins=32, seed=None) \n",
    "  \n",
    "model_rf.numTrees()\n",
    "model_rf.totalNumNodes()\n",
    "time_end=time.time()\n",
    "time_rf=(time_end - time_start)\n",
    "print(\"RF takes %d s\" %(time_rf))\n",
    " \n",
    "# Predictions on test set\n",
    "predictions = model_rf.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    " \n",
    "# first metrics\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "metrics = BinaryClassificationMetrics(labelsAndPredictions)\n",
    " \n",
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    " \n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic using sql dataframe and ml module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|  22|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|  38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|  26|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|  35|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|  35|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|  54|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male|   2|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|  27|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|  14|    1|    0|          237736|30.0708| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoder, VectorAssembler, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import *\n",
    " \n",
    "# Creatingt Spark SQL environment\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "SparkContext.setSystemProperty(\"hive.metastore.uris\", \"thrift://nn1:9083\")\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    " \n",
    "# spark is an existing SparkSession\n",
    "train = spark.read.csv(\"train.csv\", header = True)\n",
    "# Displays the content of the DataFrame to stdout\n",
    "train.show(10)\n",
    " \n",
    "# String to float on some columns of the dataset : creates a new dataset\n",
    "train = train.select(col(\"Survived\"),col(\"Sex\"),col(\"Embarked\"),col(\"Pclass\").cast(\"float\"),col(\"Age\").cast(\"float\"),col(\"SibSp\").cast(\"float\"),col(\"Fare\").cast(\"float\"))\n",
    " \n",
    "# dropping null values\n",
    "train = train.dropna()\n",
    " \n",
    "# Spliting in train and test set. Beware : It sorts the dataset\n",
    "(traindf, testdf) = train.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|prediction|         probability|\n",
      "+----------+--------------------+\n",
      "|       0.0|[0.89184996904177...|\n",
      "|       1.0|[0.04310468712642...|\n",
      "|       1.0|[0.46799611398021...|\n",
      "|       1.0|[0.04279690462562...|\n",
      "|       0.0|[0.88765386496603...|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "train = StringIndexer(inputCol=\"Sex\", outputCol=\"indexedSex\").fit(train).transform(train)\n",
    "train = StringIndexer(inputCol=\"Embarked\", outputCol=\"indexedEmbarked\").fit(train).transform(train)\n",
    " \n",
    "train = StringIndexer(inputCol=\"Survived\", outputCol=\"indexedSurvived\").fit(train).transform(train)\n",
    " \n",
    "# One Hot Encoder on indexed features\n",
    "train = OneHotEncoder(inputCol=\"indexedSex\", outputCol=\"sexVec\").transform(train)\n",
    "train = OneHotEncoder(inputCol=\"indexedEmbarked\", outputCol=\"embarkedVec\").transform(train)\n",
    " \n",
    "# Feature assembler as a vector\n",
    "train = VectorAssembler(inputCols=[\"Pclass\",\"sexVec\",\"embarkedVec\", \"Age\",\"SibSp\",\"Fare\"],outputCol=\"features\").transform(train)\n",
    " \n",
    "rf = RandomForestClassifier(labelCol=\"indexedSurvived\", featuresCol=\"features\")\n",
    " \n",
    "model = rf.fit(train)\n",
    " \n",
    "predictions = model.transform(train)\n",
    " \n",
    "# Select example rows to display.\n",
    "predictions.select(col(\"prediction\"),col(\"probability\"),).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------------------+\n",
      "|prediction|Survived|            features|\n",
      "+----------+--------+--------------------+\n",
      "|       1.0|       0|[1.0,0.0,50.0,0.0...|\n",
      "|       0.0|       0|[3.0,0.0,17.0,0.0...|\n",
      "|       1.0|       0|(7,[0,2,4],[3.0,3...|\n",
      "|       0.0|       0|(7,[0,2,4],[3.0,3...|\n",
      "|       1.0|       0|[2.0,0.0,26.0,1.0...|\n",
      "+----------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Test Error = 0.179724\n",
      "RandomForestClassificationModel (uid=RandomForestClassifier_4a078bae21c90f530455) with 20 trees\n",
      "Accuracy = 0.820276\n",
      "f1 = 0.819051\n",
      "weightedPrecision = 0.819595\n",
      "weightedRecall = 0.820276\n"
     ]
    }
   ],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "genderIndexer = StringIndexer(inputCol=\"Sex\", outputCol=\"indexedSex\")\n",
    "embarkIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"indexedEmbarked\")\n",
    " \n",
    "surviveIndexer = StringIndexer(inputCol=\"Survived\", outputCol=\"indexedSurvived\")\n",
    " \n",
    "# One Hot Encoder on indexed features\n",
    "genderEncoder = OneHotEncoder(inputCol=\"indexedSex\", outputCol=\"sexVec\")\n",
    "embarkEncoder = OneHotEncoder(inputCol=\"indexedEmbarked\", outputCol=\"embarkedVec\")\n",
    " \n",
    "# Create the vector structured data (label,features(vector))\n",
    "assembler = VectorAssembler(inputCols=[\"Pclass\",\"sexVec\",\"Age\",\"SibSp\",\"Fare\",\"embarkedVec\"],outputCol=\"features\")\n",
    " \n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedSurvived\", featuresCol=\"features\")\n",
    " \n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[surviveIndexer, genderIndexer, embarkIndexer, genderEncoder,embarkEncoder, assembler, rf]) # genderIndexer,embarkIndexer,genderEncoder,embarkEncoder,\n",
    " \n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(traindf)\n",
    " \n",
    "# Predictions\n",
    "predictions = model.transform(testdf)\n",
    " \n",
    "# Select example rows to display.\n",
    "predictions.columns \n",
    " \n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"Survived\", \"features\").show(5)\n",
    " \n",
    "# Select (prediction, true label) and compute test error\n",
    "predictions = predictions.select(col(\"Survived\").cast(\"Float\"),col(\"prediction\"))\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    " \n",
    "rfModel = model.stages[6]\n",
    "print(rfModel)  # summary only\n",
    " \n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % accuracy)\n",
    " \n",
    "evaluatorf1 = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluatorf1.evaluate(predictions)\n",
    "print(\"f1 = %g\" % f1)\n",
    " \n",
    "evaluatorwp = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "wp = evaluatorwp.evaluate(predictions)\n",
    "print(\"weightedPrecision = %g\" % wp)\n",
    " \n",
    "evaluatorwr = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "wr = evaluatorwr.evaluate(predictions)\n",
    "print(\"weightedRecall = %g\" % wr)\n",
    " \n",
    "# close sparkcontext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_now=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import newspaper\n",
    "import pandas\n",
    "from newspaper import Article\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsapi = NewsApiClient(api_key='c8b2f3f94b474cadb0b08f0bae0b1b07')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting articles for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_folder = \"news_articles\"\n",
    "if collect_now == True:\n",
    "    # any new category to be used, is to be added here\n",
    "    categories = ['business','sports','technology','entertainment']\n",
    "    date = time.strftime(\"%Y-%m-%d\")\n",
    "    if not os.path.exists(articles_folder):\n",
    "        os.makedirs(articles_folder)\n",
    "    for category in categories:\n",
    "        if not os.path.exists(articles_folder+\"/\"+category):\n",
    "            os.makedirs(articles_folder+\"/\"+category)\n",
    "        article_no = 0    \n",
    "        top_headlines = newsapi.get_top_headlines(q='', category=category, language='en', country='us', page_size=50)\n",
    "        top_headlines = pandas.DataFrame(top_headlines)\n",
    "        for ele in top_headlines['articles'].values:\n",
    "            url = ele['url']\n",
    "            article=Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            article_no = article_no + 1\n",
    "            file = open(articles_folder+\"/\"+category+\"/\"+category+\"_\"+str(date)+\"_\"+str(article_no)+\".txt\",\"w\",encoding=\"utf-8\")\n",
    "            file.write(article.text)\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting articles for testing i.e. unknown set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_folder = \"news_articles_unknown\"\n",
    "if collect_now == True:\n",
    "    date = time.strftime(\"%Y-%m-%d\")\n",
    "    if not os.path.exists(articles_folder):\n",
    "        os.makedirs(articles_folder)\n",
    "    for category in categories:\n",
    "        if not os.path.exists(articles_folder+\"/\"+category):\n",
    "            os.makedirs(articles_folder+\"/\"+category)\n",
    "        article_no = 0    \n",
    "        top_headlines = newsapi.get_top_headlines(q='', category=category, language='en', country='us', page_size=50)\n",
    "        top_headlines = pandas.DataFrame(top_headlines)\n",
    "        for ele in top_headlines['articles'].values:\n",
    "            url = ele['url']\n",
    "            article=Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            article_no = article_no + 1\n",
    "            file = open(articles_folder+\"/\"+category+\"/\"+category+\"_\"+str(date)+\"_\"+str(article_no)+\".txt\",\"w\",encoding=\"utf-8\")\n",
    "            file.write(article.text)\n",
    "            file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            sentence|\n",
      "+-----+--------------------+\n",
      "|    0|With big jaws to ...|\n",
      "|    0|CLOSE Amid a city...|\n",
      "|    0|CLOSE Twitter say...|\n",
      "|    0|PHILADELPHIA -- P...|\n",
      "|    0|The fan blade tha...|\n",
      "|    0|The Volkswagen em...|\n",
      "|    0|Activision Blizza...|\n",
      "|    0|Orlando-based Dar...|\n",
      "|    0|Copyright 2018 Th...|\n",
      "|    0|Southwest CEO: Ou...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read files and generate spark data frame\n",
    "articles_folder = \"news_articles\"\n",
    "data = []\n",
    "for subdir, dirs, files in os.walk(articles_folder):\n",
    "    for folder_name in dirs:\n",
    "        # any label associated with a new category, has to be added here\n",
    "        if folder_name == \"business\":\n",
    "            label = 0\n",
    "        elif folder_name == \"entertainment\":\n",
    "            label = 1\n",
    "        elif folder_name == \"sports\":\n",
    "            label = 2\n",
    "        elif folder_name == \"technology\":\n",
    "            label = 3\n",
    "        for filename in os.listdir(articles_folder+\"/\"+folder_name):\n",
    "            filecontent = open(articles_folder+\"/\"+folder_name+\"/\"+filename,\"r\",encoding='utf-8').read()\n",
    "            temp = (label,filecontent)\n",
    "            data.append(temp)\n",
    "            \n",
    "sentenceData = spark.createDataFrame(data,[\"label\",\"sentence\"])     \n",
    "sentenceData.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|            filtered|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|    0|With big jaws to ...|[with, big, jaws,...|[big, jaws, feed,...|\n",
      "|    0|CLOSE Amid a city...|[close, amid, a, ...|[close, amid, cit...|\n",
      "|    0|CLOSE Twitter say...|[close, twitter, ...|[close, twitter, ...|\n",
      "|    0|PHILADELPHIA -- P...|[philadelphia, --...|[philadelphia, --...|\n",
      "|    0|The fan blade tha...|[the, fan, blade,...|[fan, blade, fail...|\n",
      "|    0|The Volkswagen em...|[the, volkswagen,...|[volkswagen, emis...|\n",
      "|    0|Activision Blizza...|[activision, bliz...|[activision, bliz...|\n",
      "|    0|Orlando-based Dar...|[orlando-based, d...|[orlando-based, d...|\n",
      "|    0|Copyright 2018 Th...|[copyright, 2018,...|[copyright, 2018,...|\n",
      "|    0|Southwest CEO: Ou...|[southwest, ceo:,...|[southwest, ceo:,...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# base code for feature extraction taken from official site\n",
    "# https://spark.apache.org/docs/2.3.0/ml-features.html\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "filteredWordsData = remover.transform(wordsData)\n",
    "filteredWordsData.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing testing data i.e. unknown set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            sentence|\n",
      "+-----+--------------------+\n",
      "|    0|Footage of a prot...|\n",
      "|    0|GameStop Corp. Ch...|\n",
      "|    0|The Nomorobo robo...|\n",
      "|    0|BEIJING—Ride-hail...|\n",
      "|    0|Starbucks: No Nee...|\n",
      "|    0|We’ve seen Boston...|\n",
      "|    0|Barclays CEO Jes ...|\n",
      "|    0|Oil prices early ...|\n",
      "|    0|Banks have been w...|\n",
      "|    0|One of America's ...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read files and generate spark data frame\n",
    "articles_folder = \"news_articles_unknown\"\n",
    "data = []\n",
    "for subdir, dirs, files in os.walk(articles_folder):\n",
    "    for folder_name in dirs:\n",
    "        # any label associated with a new category, has to be added here\n",
    "        if folder_name == \"business\":\n",
    "            label = 0\n",
    "        elif folder_name == \"entertainment\":\n",
    "            label = 1\n",
    "        elif folder_name == \"sports\":\n",
    "            label = 2\n",
    "        elif folder_name == \"technology\":\n",
    "            label = 3\n",
    "        for filename in os.listdir(articles_folder+\"/\"+folder_name):\n",
    "            filecontent = open(articles_folder+\"/\"+folder_name+\"/\"+filename,\"r\",encoding='utf-8').read()\n",
    "            temp = (label,filecontent)\n",
    "            data.append(temp)\n",
    "            \n",
    "sentenceDataUnknown = spark.createDataFrame(data,[\"label\",\"sentence\"])     \n",
    "sentenceDataUnknown.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing testing data i.e. unknown set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|            filtered|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|    0|Footage of a prot...|[footage, of, a, ...|[footage, prototy...|\n",
      "|    0|GameStop Corp. Ch...|[gamestop, corp.,...|[gamestop, corp.,...|\n",
      "|    0|The Nomorobo robo...|[the, nomorobo, r...|[nomorobo, roboca...|\n",
      "|    0|BEIJING—Ride-hail...|[beijing—ride-hai...|[beijing—ride-hai...|\n",
      "|    0|Starbucks: No Nee...|[starbucks:, no, ...|[starbucks:, need...|\n",
      "|    0|We’ve seen Boston...|[we’ve, seen, bos...|[we’ve, seen, bos...|\n",
      "|    0|Barclays CEO Jes ...|[barclays, ceo, j...|[barclays, ceo, j...|\n",
      "|    0|Oil prices early ...|[oil, prices, ear...|[oil, prices, ear...|\n",
      "|    0|Banks have been w...|[banks, have, bee...|[banks, walking, ...|\n",
      "|    0|One of America's ...|[one, of, america...|[one, america's, ...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# base code for feature extraction taken from official site\n",
    "# https://spark.apache.org/docs/2.3.0/ml-features.html\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsDataUnknown = tokenizer.transform(sentenceDataUnknown)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "filteredWordsDataUnknown = remover.transform(wordsDataUnknown)\n",
    "filteredWordsDataUnknown.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction using Hasing TF and IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|       0.0|    0|\n",
      "|       0.0|    0|\n",
      "|       0.0|    0|\n",
      "|       0.0|    0|\n",
      "|       0.0|    0|\n",
      "|       0.0|    0|\n",
      "|       0.0|    0|\n",
      "|       0.0|    0|\n",
      "|       0.0|    0|\n",
      "|       0.0|    0|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "Accuracy with 5000 features for known articles: 0.7068965517241379\n",
      "Accuracy with 5000 features for unknown articles: 0.8\n",
      "Accuracy with 5500 features for known articles: 0.5625\n",
      "Accuracy with 5500 features for unknown articles: 0.775\n",
      "Accuracy with 6000 features for known articles: 0.7435897435897436\n",
      "Accuracy with 6000 features for unknown articles: 0.825\n",
      "Accuracy with 6500 features for known articles: 0.6595744680851063\n",
      "Accuracy with 6500 features for unknown articles: 0.7875\n",
      "Accuracy with 7000 features for known articles: 0.6730769230769231\n",
      "Accuracy with 7000 features for unknown articles: 0.8\n",
      "Accuracy with 7500 features for known articles: 0.6122448979591837\n",
      "Accuracy with 7500 features for unknown articles: 0.7625\n",
      "Accuracy with 8000 features for known articles: 0.7192982456140351\n",
      "Accuracy with 8000 features for unknown articles: 0.7875\n",
      "Accuracy with 8500 features for known articles: 0.7\n",
      "Accuracy with 8500 features for unknown articles: 0.8\n",
      "Accuracy with 9000 features for known articles: 0.7272727272727273\n",
      "Accuracy with 9000 features for unknown articles: 0.8\n",
      "Accuracy with 9500 features for known articles: 0.8378378378378378\n",
      "Accuracy with 9500 features for unknown articles: 0.8\n",
      "Accuracy with 10000 features for known articles: 0.6086956521739131\n",
      "Accuracy with 10000 features for unknown articles: 0.7\n"
     ]
    }
   ],
   "source": [
    "# base code for classifiers taken from official site\n",
    "# https://spark.apache.org/docs/2.3.0/ml-classification-regression.html\n",
    "show_predictions=True\n",
    "for i in range(5000,10001,500):\n",
    "    hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=i)\n",
    "    featurizedData = hashingTF.transform(filteredWordsData)\n",
    "\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(featurizedData)\n",
    "    rescaledData = idfModel.transform(featurizedData)\n",
    "    \n",
    "    (trainingData, testData) = rescaledData.randomSplit([0.8, 0.2])  \n",
    "    \n",
    "    hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=i)\n",
    "    featurizedDataUnknown = hashingTF.transform(filteredWordsDataUnknown)\n",
    "\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(featurizedDataUnknown)\n",
    "    rescaledDataUnknown = idfModel.transform(featurizedDataUnknown)\n",
    "    \n",
    "    unknownData = rescaledDataUnknown\n",
    "\n",
    "    # Train a RandomForest model.\n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=200)\n",
    "\n",
    "    pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "    # Train model.  This also runs the indexers.\n",
    "    model = pipeline.fit(trainingData)\n",
    "\n",
    "    # Make predictions for known dataset.\n",
    "    predictions = model.transform(testData)\n",
    "    if show_predictions == True:\n",
    "        predictions.select(\"prediction\",\"label\").show(10)\n",
    "        show_predictions=False\n",
    "\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Accuracy with \"+str(i)+\" features for known articles: \"+str(accuracy))\n",
    "    \n",
    "     # Make predictions for unknown dataset.\n",
    "    predictions = model.transform(unknownData)\n",
    "    if show_predictions == True:\n",
    "        predictions.select(\"prediction\",\"label\").show(10)\n",
    "        show_predictions=False\n",
    "\n",
    "    # Select (prediction, true label) and compute test error\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Accuracy with \"+str(i)+\" features for unknown articles: \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            filtered|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[big, jaws, feed,...|(9500,[24,34,37,4...|\n",
      "|    0|[close, amid, cit...|(9500,[30,37,69,2...|\n",
      "|    0|[close, twitter, ...|(9500,[37,208,256...|\n",
      "|    0|[philadelphia, --...|(9500,[125,135,19...|\n",
      "|    0|[fan, blade, fail...|(9500,[37,46,94,2...|\n",
      "|    0|[volkswagen, emis...|(9500,[33,80,543,...|\n",
      "|    0|[activision, bliz...|(9500,[24,66,161,...|\n",
      "|    0|[orlando-based, d...|(9500,[8,109,116,...|\n",
      "|    0|[copyright, 2018,...|(9500,[1,58,92,16...|\n",
      "|    0|[southwest, ceo:,...|(9500,[47,72,104,...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaledData.select(\"label\",\"filtered\",\"features\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naives Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|label|\n",
      "+----------+-----+\n",
      "|       3.0|    0|\n",
      "|       0.0|    0|\n",
      "|       0.0|    0|\n",
      "|       3.0|    0|\n",
      "|       0.0|    0|\n",
      "|       0.0|    0|\n",
      "|       0.0|    0|\n",
      "|       0.0|    0|\n",
      "|       1.0|    0|\n",
      "|       0.0|    0|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "Accuracy with 10000 features for known articles: 0.8378378378378378\n",
      "Accuracy with 10000 features for unknown articles: 0.875\n",
      "Accuracy with 10500 features for known articles: 0.8958333333333334\n",
      "Accuracy with 10500 features for unknown articles: 0.8125\n",
      "Accuracy with 11000 features for known articles: 0.9387755102040817\n",
      "Accuracy with 11000 features for unknown articles: 0.9125\n",
      "Accuracy with 11500 features for known articles: 0.8863636363636364\n",
      "Accuracy with 11500 features for unknown articles: 0.85\n",
      "Accuracy with 12000 features for known articles: 0.9245283018867925\n",
      "Accuracy with 12000 features for unknown articles: 0.8375\n",
      "Accuracy with 12500 features for known articles: 0.8723404255319149\n",
      "Accuracy with 12500 features for unknown articles: 0.85\n",
      "Accuracy with 13000 features for known articles: 0.8444444444444444\n",
      "Accuracy with 13000 features for unknown articles: 0.9125\n",
      "Accuracy with 13500 features for known articles: 0.9347826086956522\n",
      "Accuracy with 13500 features for unknown articles: 0.8625\n",
      "Accuracy with 14000 features for known articles: 0.84\n",
      "Accuracy with 14000 features for unknown articles: 0.9125\n",
      "Accuracy with 14500 features for known articles: 0.8888888888888888\n",
      "Accuracy with 14500 features for unknown articles: 0.85\n",
      "Accuracy with 15000 features for known articles: 0.82\n",
      "Accuracy with 15000 features for unknown articles: 0.8875\n",
      "Accuracy with 15500 features for known articles: 0.9772727272727273\n",
      "Accuracy with 15500 features for unknown articles: 0.9\n",
      "Accuracy with 16000 features for known articles: 0.9069767441860465\n",
      "Accuracy with 16000 features for unknown articles: 0.9\n",
      "Accuracy with 16500 features for known articles: 0.88\n",
      "Accuracy with 16500 features for unknown articles: 0.9\n",
      "Accuracy with 17000 features for known articles: 0.9411764705882353\n",
      "Accuracy with 17000 features for unknown articles: 0.9125\n",
      "Accuracy with 17500 features for known articles: 0.9183673469387755\n",
      "Accuracy with 17500 features for unknown articles: 0.9125\n",
      "Accuracy with 18000 features for known articles: 0.9285714285714286\n",
      "Accuracy with 18000 features for unknown articles: 0.8625\n",
      "Accuracy with 18500 features for known articles: 0.8333333333333334\n",
      "Accuracy with 18500 features for unknown articles: 0.875\n",
      "Accuracy with 19000 features for known articles: 0.9302325581395349\n",
      "Accuracy with 19000 features for unknown articles: 0.8875\n",
      "Accuracy with 19500 features for known articles: 0.9285714285714286\n",
      "Accuracy with 19500 features for unknown articles: 0.9125\n",
      "Accuracy with 20000 features for known articles: 0.9230769230769231\n",
      "Accuracy with 20000 features for unknown articles: 0.8875\n"
     ]
    }
   ],
   "source": [
    "show_predictions=True\n",
    "for i in range(10000,20001,500):\n",
    "    hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=i)\n",
    "    featurizedData = hashingTF.transform(filteredWordsData)\n",
    "\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(featurizedData)\n",
    "    rescaledData = idfModel.transform(featurizedData)\n",
    "    \n",
    "    splits = rescaledData.randomSplit([0.8, 0.2])\n",
    "    train = splits[0]\n",
    "    test = splits[1]\n",
    "    \n",
    "    hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=i)\n",
    "    featurizedDataUnknown = hashingTF.transform(filteredWordsDataUnknown)\n",
    "\n",
    "    idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(featurizedDataUnknown)\n",
    "    rescaledDataUnknown = idfModel.transform(featurizedDataUnknown)\n",
    "    \n",
    "    unknown = rescaledDataUnknown\n",
    "\n",
    "    # create the trainer and set its parameters\n",
    "    nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "    # train the model\n",
    "    model = nb.fit(train)\n",
    "\n",
    "    # select example rows to display.\n",
    "    predictions = model.transform(test)\n",
    "    if show_predictions == True:\n",
    "        predictions.select(\"prediction\",\"label\").show(10)\n",
    "        show_predictions=False    \n",
    "\n",
    "    # compute accuracy on the test set\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                                  metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Accuracy with \"+str(i)+\" features for known articles: \"+str(accuracy))\n",
    "    \n",
    "    # select example rows to display.\n",
    "    predictions = model.transform(unknown)\n",
    "    if show_predictions == True:\n",
    "        predictions.select(\"prediction\",\"label\").show(10)\n",
    "        show_predictions=False    \n",
    "\n",
    "    # compute accuracy on the test set\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                                  metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Accuracy with \"+str(i)+\" features for unknown articles: \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"C:/Users/SATVIN~1/ANACON~1/Scripts/pandoc\" +RTS -K512m -RTS ReportLab3.utf8.md --to html4 --from markdown+autolink_bare_uris+ascii_identifiers+tex_math_single_backslash --output ReportLab3.html --smart --email-obfuscation none --self-contained --standalone --section-divs --template \"C:\\Users\\Satvinder\\Documents\\R\\win-library\\3.4\\rmarkdown\\rmd\\h\\default.html\" --no-highlight --variable highlightjs=1 --variable \"theme:bootstrap\" --include-in-header \"C:\\Users\\SATVIN~1\\AppData\\Local\\Temp\\RtmpEpvbq4\\rmarkdown-str414c42c0423f.html\" --mathjax --variable \"mathjax-url:https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Output created: ReportLab3.html\n"
     ]
    }
   ],
   "source": [
    "#Uncomment below line, change kernel to R and execute to regenerate html report\n",
    "#rmarkdown::render(\"ReportLab3.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
